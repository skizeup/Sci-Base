[
  {
    "title": "MECCH: Metapath Context Convolution-based Heterogeneous Graph Neural Networks",
    "authors": [
      "Xinyu Fu",
      "Irwin King"
    ],
    "year": 2022,
    "url": "http://arxiv.org/abs/2211.12792v2",
    "abstract": "Heterogeneous graph neural networks (HGNNs) were proposed for representation learning on structural data with multiple types of nodes and edges. To deal with the performance degradation issue when HGNNs become deep, researchers combine metapaths into HGNNs to associate nodes closely related in semantics but far apart in the graph. However, existing metapath-based models suffer from either information loss or high computation costs. To address these problems, we present a novel Metapath Context Convolution-based Heterogeneous Graph Neural Network (MECCH). MECCH leverages metapath contexts, a new kind of graph structure that facilitates lossless node information aggregation while avoiding any redundancy. Specifically, MECCH applies three novel components after feature preprocessing to extract comprehensive information from the input graph efficiently: (1) metapath context construction, (2) metapath context encoder, and (3) convolutional metapath fusion. Experiments on five real-world heterogeneous graph datasets for node classification and link prediction show that MECCH achieves superior prediction accuracy compared with state-of-the-art baselines with improved computational efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2211.12792v2",
    "source": "arxiv",
    "tags": [
      "cs.LG"
    ],
    "doi": "10.1016/j.neunet.2023.11.030"
  },
  {
    "title": "Transformers are Graph Neural Networks",
    "authors": [
      "Chaitanya K. Joshi"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2506.22084v1",
    "abstract": "We establish connections between the Transformer architecture, originally introduced for natural language processing, and Graph Neural Networks (GNNs) for representation learning on graphs. We show how Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure. Thus, Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Despite this mathematical connection to GNNs, Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing. This leads to the perspective that Transformers are GNNs currently winning the hardware lottery.",
    "pdf_url": "https://arxiv.org/pdf/2506.22084v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "Fast and Deep Graph Neural Networks",
    "authors": [
      "Claudio Gallicchio",
      "Alessio Micheli"
    ],
    "year": 2019,
    "url": "http://arxiv.org/abs/1911.08941v1",
    "abstract": "We address the efficiency issue for the construction of a deep graph neural network (GNN). The approach exploits the idea of representing each input graph as a fixed point of a dynamical system (implemented through a recurrent neural network), and leverages a deep architectural organization of the recurrent units. Efficiency is gained by many aspects, including the use of small and very sparse networks, where the weights of the recurrent units are left untrained under the stability condition introduced in this work. This can be viewed as a way to study the intrinsic power of the architecture of a deep GNN, and also to provide insights for the set-up of more complex fully-trained models. Through experimental results, we show that even without training of the recurrent connections, the architecture of small deep GNN is surprisingly able to achieve or improve the state-of-the-art performance on a significant set of tasks in the field of graphs classification.",
    "pdf_url": "https://arxiv.org/pdf/1911.08941v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "math.DS",
      "stat.ML"
    ]
  },
  {
    "title": "Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set",
    "authors": [
      "Maria Chiara Angelini",
      "Federico Ricci-Tersenghi"
    ],
    "year": 2022,
    "url": "http://arxiv.org/abs/2206.13211v2",
    "abstract": "The recent work ``Combinatorial Optimization with Physics-Inspired Graph Neural Networks'' [Nat Mach Intell 4 (2022) 367] introduces a physics-inspired unsupervised Graph Neural Network (GNN) to solve combinatorial optimization problems on sparse graphs. To test the performances of these GNNs, the authors of the work show numerical results for two fundamental problems: maximum cut and maximum independent set (MIS). They conclude that \"the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables.\" In this comment, we show that a simple greedy algorithm, running in almost linear time, can find solutions for the MIS problem of much better quality than the GNN. The greedy algorithm is faster by a factor of $10^4$ with respect to the GNN for problems with a million variables. We do not see any good reason for solving the MIS with these GNN, as well as for using a sledgehammer to crack nuts. In general, many claims of superiority of neural networks in solving combinatorial problems are at risk of being not solid enough, since we lack standard benchmarks based on really hard problems. We propose one of such hard benchmarks, and we hope to see future neural network optimizers tested on these problems before any claim of superiority is made.",
    "pdf_url": "https://arxiv.org/pdf/2206.13211v2",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "math.OC"
    ],
    "doi": "10.1038/s42256-022-00589-y"
  },
  {
    "title": "Missing Data Imputation with Adversarially-trained Graph Convolutional Networks",
    "authors": [
      "Indro Spinelli",
      "Simone Scardapane",
      "Aurelio Uncini"
    ],
    "year": 2019,
    "url": "http://arxiv.org/abs/1905.01907v2",
    "abstract": "Missing data imputation (MDI) is a fundamental problem in many scientific disciplines. Popular methods for MDI use global statistics computed from the entire data set (e.g., the feature-wise medians), or build predictive models operating independently on every instance. In this paper we propose a more general framework for MDI, leveraging recent work in the field of graph neural networks (GNNs). We formulate the MDI task in terms of a graph denoising autoencoder, where each edge of the graph encodes the similarity between two patterns. A GNN encoder learns to build intermediate representations for each example by interleaving classical projection layers and locally combining information between neighbors, while another decoding GNN learns to reconstruct the full imputed data set from this intermediate embedding. In order to speed-up training and improve the performance, we use a combination of multiple losses, including an adversarial loss implemented with the Wasserstein metric and a gradient penalty. We also explore a few extensions to the basic architecture involving the use of residual connections between layers, and of global statistics computed from the data set to improve the accuracy. On a large experimental evaluation, we show that our method robustly outperforms state-of-the-art approaches for MDI, especially for large percentages of missing values.",
    "pdf_url": "https://arxiv.org/pdf/1905.01907v2",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "stat.ML"
    ],
    "doi": "10.1016/j.neunet.2020.06.005"
  }
]
