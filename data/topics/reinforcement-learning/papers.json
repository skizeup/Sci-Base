[
  {
    "title": "Playing Atari with Deep Reinforcement Learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"],
    "year": 2013,
    "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games, with no adjustment of the architecture or learning algorithm.",
    "url": "https://arxiv.org/abs/1312.5602",
    "pdf_url": "https://arxiv.org/pdf/1312.5602",
    "source": "arxiv",
    "tags": ["dqn", "atari", "deep-rl"],
    "doi": "10.48550/arXiv.1312.5602"
  },
  {
    "title": "Mastering the Game of Go with Deep Neural Networks and Tree Search",
    "authors": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"],
    "year": 2016,
    "abstract": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. We introduce a new approach to computer Go that uses value networks to evaluate board positions and policy networks to select moves, trained by a novel combination of supervised learning from human expert games and reinforcement learning from games of self-play.",
    "url": "https://www.nature.com/articles/nature16961",
    "source": "manual",
    "tags": ["alphago", "mcts", "deep-rl", "go"],
    "doi": "10.1038/nature16961"
  },
  {
    "title": "Proximal Policy Optimization Algorithms",
    "authors": ["John Schulman", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov"],
    "year": 2017,
    "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates.",
    "url": "https://arxiv.org/abs/1707.06347",
    "pdf_url": "https://arxiv.org/pdf/1707.06347",
    "source": "arxiv",
    "tags": ["ppo", "policy-gradient", "deep-rl"],
    "doi": "10.48550/arXiv.1707.06347"
  },
  {
    "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
    "authors": ["David Silver", "Thomas Hubert", "Julian Schrittwieser", "Ioannis Antonoglou", "Matthew Lai", "Arthur Guez", "Marc Lanctot", "Laurent Sifre", "Dharshan Kumaran", "Thore Graepel", "Timothy Lillicrap", "Karen Simonyan", "Demis Hassabis"],
    "year": 2017,
    "abstract": "The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions. We introduce AlphaZero, a general reinforcement learning algorithm that masters chess, shogi, and Go through self-play, given no domain knowledge except the game rules.",
    "url": "https://arxiv.org/abs/1712.01815",
    "pdf_url": "https://arxiv.org/pdf/1712.01815",
    "source": "arxiv",
    "tags": ["alphazero", "self-play", "mcts", "deep-rl"],
    "doi": "10.48550/arXiv.1712.01815"
  },
  {
    "title": "Training Language Models to Follow Instructions with Human Feedback",
    "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "Paul Christiano", "Jan Leike", "Ryan Lowe"],
    "year": 2022,
    "abstract": "Making language models bigger does not inherently make them better at following a user's intent. Large language models can generate outputs that are untruthful, toxic, or simply not helpful. We show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback (RLHF).",
    "url": "https://arxiv.org/abs/2203.02155",
    "pdf_url": "https://arxiv.org/pdf/2203.02155",
    "source": "arxiv",
    "tags": ["rlhf", "alignment", "llm", "instruction-following"],
    "doi": "10.48550/arXiv.2203.02155"
  },
  {
    "title": "Continuous Control with Deep Reinforcement Learning",
    "authors": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"],
    "year": 2015,
    "abstract": "We adapt the ideas underlying the success of Deep Q-Network to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Our algorithm, DDPG, can robustly solve a variety of continuous control problems.",
    "url": "https://arxiv.org/abs/1509.02971",
    "pdf_url": "https://arxiv.org/pdf/1509.02971",
    "source": "arxiv",
    "tags": ["ddpg", "actor-critic", "continuous-control", "deep-rl"],
    "doi": "10.48550/arXiv.1509.02971"
  },
  {
    "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
    "authors": ["Julian Schrittwieser", "Ioannis Antonoglou", "Thomas Hubert", "Karen Simonyan", "Laurent Sifre", "Simon Schmitt", "Arthur Guez", "Edward Lockhart", "Demis Hassabis", "Thore Graepel", "Timothy Lillicrap", "David Silver"],
    "year": 2020,
    "abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. We introduce MuZero, a new approach to model-based reinforcement learning that achieves state-of-the-art performance in Atari, Go, chess and shogi, without any knowledge of the environment dynamics.",
    "url": "https://arxiv.org/abs/1911.08265",
    "pdf_url": "https://arxiv.org/pdf/1911.08265",
    "source": "arxiv",
    "tags": ["muzero", "model-based", "planning", "deep-rl"],
    "doi": "10.1038/s41586-020-03051-4"
  }
]
