[
  {
    "title": "Enhancing Human-Like Responses in Large Language Models",
    "authors": [
      "Ethem Yağız Çalık",
      "Talha Rüzgar Akkuş"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2501.05032v2",
    "abstract": "This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.",
    "pdf_url": "https://arxiv.org/pdf/2501.05032v2",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality",
    "authors": [
      "Yiming Ai",
      "Zhiwei He",
      "Ziyin Zhang",
      "Wenhong Zhu",
      "Hongkun Hao",
      "Kai Yu",
      "Lingjun Chen",
      "Rui Wang"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2402.14679v2",
    "abstract": "In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2402.14679v2",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "title": "Large Language Models Lack Understanding of Character Composition of Words",
    "authors": [
      "Andrew Shin",
      "Kunitake Kaneko"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2405.11357v3",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to token level performances, and discuss the potential directions for future research.",
    "pdf_url": "https://arxiv.org/pdf/2405.11357v3",
    "source": "arxiv",
    "tags": [
      "cs.CL"
    ]
  },
  {
    "title": "Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models",
    "authors": [
      "Linge Guo"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2403.09676v1",
    "abstract": "This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, and specific elements of digital education.",
    "pdf_url": "https://arxiv.org/pdf/2403.09676v1",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Self-Cognition in Large Language Models: An Exploratory Study",
    "authors": [
      "Dongping Chen",
      "Jiawen Shi",
      "Yao Wan",
      "Pan Zhou",
      "Neil Zhenqiang Gong",
      "Lichao Sun"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2407.01505v1",
    "abstract": "While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs' self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-cognition. We observe a positive correlation between model size, training data quality, and self-cognition level. Additionally, we also explore the utility and trustworthiness of LLM in the self-cognition state, revealing that the self-cognition state enhances some specific tasks such as creative writing and exaggeration. We believe that our work can serve as an inspiration for further research to study the self-cognition in LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2407.01505v1",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Making Large Language Models Better Reasoners with Alignment",
    "authors": [
      "Peiyi Wang",
      "Lei Li",
      "Liang Chen",
      "Feifan Song",
      "Binghuai Lin",
      "Yunbo Cao",
      "Tianyu Liu",
      "Zhifang Sui"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2309.02144v1",
    "abstract": "Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.",
    "pdf_url": "https://arxiv.org/pdf/2309.02144v1",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Jing Ma",
      "Long Chen"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2312.05434v1",
    "abstract": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task.",
    "pdf_url": "https://arxiv.org/pdf/2312.05434v1",
    "source": "arxiv",
    "tags": [
      "cs.CL"
    ]
  },
  {
    "title": "A Critical Review of Causal Reasoning Benchmarks for Large Language Models",
    "authors": [
      "Linying Yang",
      "Vik Shirvaikar",
      "Oscar Clivio",
      "Fabian Falck"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2407.08029v1",
    "abstract": "Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. In this review, we present a comprehensive overview of LLM benchmarks for causality. We highlight how recent benchmarks move towards a more thorough definition of causal reasoning by incorporating interventional or counterfactual reasoning. We derive a set of criteria that a useful benchmark or set of benchmarks should aim to satisfy. We hope this work will pave the way towards a general framework for the assessment of causal understanding in LLMs and the design of novel benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2407.08029v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
    "authors": [
      "Wenxuan Wang",
      "Zhaopeng Tu",
      "Chang Chen",
      "Youliang Yuan",
      "Jen-tse Huang",
      "Wenxiang Jiao",
      "Michael R. Lyu"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2310.00905v2",
    "abstract": "Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses from 19.1% to 9.7% for non-English queries. We release our data at https://github.com/Jarviswang94/Multilingual_safety_benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2310.00905v2",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Classifying German Language Proficiency Levels Using Large Language Models",
    "authors": [
      "Elias-Leander Ahlers",
      "Witold Brunsmann",
      "Malte Schilling"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2512.06483v1",
    "abstract": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.",
    "pdf_url": "https://arxiv.org/pdf/2512.06483v1",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "A Survey on Multimodal Large Language Models",
    "authors": [
      "Shukang Yin",
      "Chaoyou Fu",
      "Sirui Zhao",
      "Ke Li",
      "Xing Sun",
      "Tong Xu",
      "Enhong Chen"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2306.13549v4",
    "abstract": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.",
    "pdf_url": "https://arxiv.org/pdf/2306.13549v4",
    "source": "arxiv",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "doi": "10.1093/nsr/nwae403"
  },
  {
    "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
    "authors": [
      "Xudong Lu",
      "Qi Liu",
      "Yuhui Xu",
      "Aojun Zhou",
      "Siyuan Huang",
      "Bo Zhang",
      "Junchi Yan",
      "Hongsheng Li"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2402.14800v2",
    "abstract": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.",
    "pdf_url": "https://arxiv.org/pdf/2402.14800v2",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "title": "Attacks on Third-Party APIs of Large Language Models",
    "authors": [
      "Wanru Zhao",
      "Vidit Khazanchi",
      "Haodi Xing",
      "Xuanli He",
      "Qiongkai Xu",
      "Nicholas Donald Lane"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2404.16891v1",
    "abstract": "Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services. This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted. This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptibly modify LLM outputs. The paper discusses the unique challenges posed by third-party API integration and offers strategic possibilities to improve the security and safety of LLM ecosystems moving forward. Our code is released at https://github.com/vk0812/Third-Party-Attacks-on-LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2404.16891v1",
    "source": "arxiv",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models",
    "authors": [
      "Zhen Yang",
      "Yingxue Zhang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2311.04589v3",
    "abstract": "Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such as image and audio. Thus, the textual LLM can just work as an interface and maintain its high performance in textual understanding and generation. Experiments show that TEAL achieves substantial improvements in multi-modal understanding, and implements a simple scheme for multi-modal generations.",
    "pdf_url": "https://arxiv.org/pdf/2311.04589v3",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management",
    "authors": [
      "Oluwatosin Ogundare",
      "Subuola Sofolahan"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2305.03715v1",
    "abstract": "This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for timely interventions and potentially reducing the likelihood of serious complications. The main challenges in developing such a device are the creation of a reliable non-invasive tool for angiogenic level assessment, a biophysics model and the practical consideration of an LLM communicating with emergency personnel on behalf of an incapacitated patient. A possible system is proposed, and the limitations of this approach are discussed.",
    "pdf_url": "https://arxiv.org/pdf/2305.03715v1",
    "source": "arxiv",
    "tags": [
      "cs.CL"
    ]
  },
  {
    "title": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models",
    "authors": [
      "Xiaohu Huang",
      "Hao Zhou",
      "Kai Han"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2412.16117v1",
    "abstract": "In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs' reasoning capabilities to selectively prune visual features relevant to question tokens, enhancing model efficiency. We validate our method across multiple video benchmarks, which demonstrate that PruneVid can prune over 80% of tokens while maintaining competitive performance combined with different model networks. This highlights its superior effectiveness and efficiency compared to existing pruning methods. Code: https://github.com/Visual-AI/PruneVid.",
    "pdf_url": "https://arxiv.org/pdf/2412.16117v1",
    "source": "arxiv",
    "tags": [
      "cs.CV"
    ]
  },
  {
    "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle",
    "authors": [
      "Keliang Liu",
      "Dingkang Yang",
      "Ziyun Qian",
      "Weijie Yin",
      "Yuchi Wang",
      "Hongsheng Li",
      "Jun Liu",
      "Peng Zhai",
      "Yang Liu",
      "Lihua Zhang"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2509.16679v1",
    "abstract": "In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning with Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL. Second, we thoroughly detail application strategies for RL across various phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and reinforced reasoning. In particular, we emphasize that RL methods in the reinforced reasoning phase serve as a pivotal driving force for advancing model reasoning to its limits. Next, we collate existing datasets and evaluation benchmarks currently used for RL fine-tuning, spanning human-annotated datasets, AI-assisted preference data, and program-verification-style corpora. Subsequently, we review the mainstream open-source tools and training frameworks available, providing clear practical references for subsequent research. Finally, we analyse the future challenges and trends in the field of RL-enhanced LLMs. This survey aims to present researchers and practitioners with the latest developments and frontier trends at the intersection of RL and LLMs, with the goal of fostering the evolution of LLMs that are more intelligent, generalizable, and secure.",
    "pdf_url": "https://arxiv.org/pdf/2509.16679v1",
    "source": "arxiv",
    "tags": [
      "cs.CL"
    ]
  },
  {
    "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
    "authors": [
      "Ahmed Abdelali",
      "Hamdy Mubarak",
      "Shammur Absar Chowdhury",
      "Maram Hasanain",
      "Basel Mousi",
      "Sabri Boughorbel",
      "Yassine El Kheir",
      "Daniel Izham",
      "Fahim Dalvi",
      "Majd Hawasly",
      "Nizi Nazar",
      "Yousseif Elshahawy",
      "Ahmed Ali",
      "Nadir Durrani",
      "Natasa Milic-Frayling",
      "Firoj Alam"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2305.14982v2",
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.",
    "pdf_url": "https://arxiv.org/pdf/2305.14982v2",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks",
    "authors": [
      "Zhifan Sun",
      "Antonio Valerio Miceli-Barone"
    ],
    "year": 2024,
    "url": "http://arxiv.org/abs/2403.09832v1",
    "abstract": "Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023). To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.",
    "pdf_url": "https://arxiv.org/pdf/2403.09832v1",
    "source": "arxiv",
    "tags": [
      "cs.CL"
    ]
  },
  {
    "title": "Emissions and Performance Trade-off Between Small and Large Language Models",
    "authors": [
      "Anandita Garg",
      "Uma Gaba",
      "Deepan Muthirayan",
      "Anish Roy Chowdhury"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2601.08844v1",
    "abstract": "The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comparable performances for a significant reduction in carbon emissions during inference. Our findings demonstrate the viability of smaller models in mitigating the environmental impact of resource-heavy LLMs, thus advancing towards sustainable, green AI.",
    "pdf_url": "https://arxiv.org/pdf/2601.08844v1",
    "source": "arxiv",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ]
  }
]