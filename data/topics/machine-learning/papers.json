[
  {
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "year": 2017,
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
    "url": "https://arxiv.org/abs/1706.03762",
    "pdf_url": "https://arxiv.org/pdf/1706.03762",
    "source": "arxiv",
    "tags": ["transformers", "attention", "nlp", "deep-learning"],
    "doi": "10.48550/arXiv.1706.03762"
  },
  {
    "title": "A Few Useful Things to Know About Machine Learning",
    "authors": ["Pedro Domingos"],
    "year": 2012,
    "abstract": "Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. This article summarizes twelve key lessons that machine learning researchers and practitioners have learned, including pitfalls to avoid and important issues to focus on.",
    "url": "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf",
    "source": "manual",
    "tags": ["survey", "fundamentals", "best-practices"]
  },
  {
    "title": "Random Forests",
    "authors": ["Leo Breiman"],
    "year": 2001,
    "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges to a limit as the number of trees in the forest becomes large. The generalization error depends on the strength of the individual trees and the correlation between them.",
    "url": "https://link.springer.com/article/10.1023/A:1010933404324",
    "source": "manual",
    "tags": ["ensemble-methods", "classification", "regression"],
    "doi": "10.1023/A:1010933404324"
  },
  {
    "title": "Support-Vector Networks",
    "authors": ["Corinna Cortes", "Vladimir Vapnik"],
    "year": 1995,
    "abstract": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed.",
    "url": "https://link.springer.com/article/10.1007/BF00994018",
    "source": "manual",
    "tags": ["svm", "classification", "kernel-methods"],
    "doi": "10.1007/BF00994018"
  },
  {
    "title": "Gradient-Based Learning Applied to Document Recognition",
    "authors": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"],
    "year": 1998,
    "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters with minimal preprocessing.",
    "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf",
    "source": "manual",
    "tags": ["cnn", "deep-learning", "computer-vision", "backpropagation"],
    "doi": "10.1109/5.726791"
  },
  {
    "title": "An Introduction to Statistical Learning",
    "authors": ["Gareth James", "Daniela Witten", "Trevor Hastie", "Robert Tibshirani"],
    "year": 2013,
    "abstract": "This book provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics. It covers supervised and unsupervised learning, with a focus on practical applications in R.",
    "url": "https://www.statlearning.com/",
    "source": "manual",
    "tags": ["textbook", "statistics", "fundamentals", "supervised-learning"]
  },
  {
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "authors": ["Sergey Ioffe", "Christian Szegedy"],
    "year": 2015,
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.",
    "url": "https://arxiv.org/abs/1502.03167",
    "pdf_url": "https://arxiv.org/pdf/1502.03167",
    "source": "arxiv",
    "tags": ["optimization", "deep-learning", "training"],
    "doi": "10.48550/arXiv.1502.03167"
  },
  {
    "title": "Adam: A Method for Stochastic Optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba"],
    "year": 2014,
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters.",
    "url": "https://arxiv.org/abs/1412.6980",
    "pdf_url": "https://arxiv.org/pdf/1412.6980",
    "source": "arxiv",
    "tags": ["optimization", "gradient-descent", "deep-learning"],
    "doi": "10.48550/arXiv.1412.6980"
  },
  {
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"],
    "year": 2014,
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. We propose dropout, a technique for addressing this problem. The key idea is to randomly drop units along with their connections from the neural network during training. This prevents units from co-adapting too much.",
    "url": "https://jmlr.org/papers/v15/srivastava14a.html",
    "source": "manual",
    "tags": ["regularization", "deep-learning", "overfitting"]
  },
  {
    "title": "XGBoost: A Scalable Tree Boosting System",
    "authors": ["Tianqi Chen", "Carlos Guestrin"],
    "year": 2016,
    "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges.",
    "url": "https://arxiv.org/abs/1603.02754",
    "pdf_url": "https://arxiv.org/pdf/1603.02754",
    "source": "arxiv",
    "tags": ["boosting", "ensemble-methods", "classification", "regression"],
    "doi": "10.1145/2939672.2939785"
  }
]
