[
  {
    "title": "Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms",
    "authors": [
      "Ehsan Hallaji",
      "Roozbeh Razavi-Far",
      "Mehrdad Saif"
    ],
    "year": 2022,
    "url": "http://arxiv.org/abs/2207.02337v1",
    "abstract": "The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use more practical. One of the most significant advancements in this domain is the incorporation of transfer learning into federated learning, which overcomes fundamental constraints of primary federated learning, particularly in terms of security. This chapter performs a comprehensive survey on the intersection of federated and transfer learning from a security point of view. The main goal of this study is to uncover potential vulnerabilities and defense mechanisms that might compromise the privacy and performance of systems that use federated and transfer learning.",
    "pdf_url": "https://arxiv.org/pdf/2207.02337v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.DC"
    ],
    "doi": "10.1007/978-3-031-11748-0_3"
  },
  {
    "title": "Transfer Learning Toolkit: Primers and Benchmarks",
    "authors": [
      "Fuzhen Zhuang",
      "Keyu Duan",
      "Tongjia Guo",
      "Yongchun Zhu",
      "Dongbo Xi",
      "Zhiyuan Qi",
      "Qing He"
    ],
    "year": 2019,
    "url": "http://arxiv.org/abs/1911.08967v1",
    "abstract": "The transfer learning toolkit wraps the codes of 17 transfer learning models and provides integrated interfaces, allowing users to use those models by calling a simple function. It is easy for primary researchers to use this toolkit and to choose proper models for real-world applications. The toolkit is written in Python and distributed under MIT open source license. In this paper, the current state of this toolkit is described and the necessary environment setting and usage are introduced.",
    "pdf_url": "https://arxiv.org/pdf/1911.08967v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "title": "Predicting concentration levels of air pollutants by transfer learning and recurrent neural network",
    "authors": [
      "Iat Hang Fong",
      "Tengyue Li",
      "Simon Fong",
      "Raymond K. Wong",
      "Antonio J. Tall√≥n-Ballesteros"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2502.01654v1",
    "abstract": "Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2502.01654v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.NE",
      "physics.ao-ph"
    ]
  },
  {
    "title": "Transfer Learning and Organic Computing for Autonomous Vehicles",
    "authors": [
      "Christofer Fellicious"
    ],
    "year": 2018,
    "url": "http://arxiv.org/abs/1808.05443v1",
    "abstract": "Autonomous Vehicles(AV) are one of the brightest promises of the future which would help cut down fatalities and improve travel time while working in harmony. Autonomous vehicles will face with challenging situations and experiences not seen before. These experiences should be converted to knowledge and help the vehicle prepare better in the future. Online Transfer Learning will help transferring prior knowledge to a new task and also keep the knowledge updated as the task evolves. This paper presents the different methods of transfer learning, online transfer learning and organic computing that could be adapted to the domain of autonomous vehicles.",
    "pdf_url": "https://arxiv.org/pdf/1808.05443v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ]
  },
  {
    "title": "Transfer Learning with Pre-trained Conditional Generative Models",
    "authors": [
      "Shin'ya Yamaguchi",
      "Sekitoshi Kanai",
      "Atsutoshi Kumagai",
      "Daiki Chijiwa",
      "Hisashi Kashima"
    ],
    "year": 2022,
    "url": "http://arxiv.org/abs/2204.12833v3",
    "abstract": "Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) source and target task label spaces overlap, (ii) source datasets are available, and (iii) target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL).",
    "pdf_url": "https://arxiv.org/pdf/2204.12833v3",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "doi": "10.1007/s10994-025-06748-7"
  },
  {
    "title": "Transferability in Deep Learning: A Survey",
    "authors": [
      "Junguang Jiang",
      "Yang Shu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "year": 2022,
    "url": "http://arxiv.org/abs/2201.05867v1",
    "abstract": "The success of deep learning algorithms generally depends on large-scale data, while humans appear to have inherent ability of knowledge transfer, by recognizing and applying relevant knowledge from previous learning experiences when encountering and solving unseen tasks. Such an ability to acquire and reuse knowledge is known as transferability in deep learning. It has formed the long-term quest towards making deep learning as data-efficient as human learning, and has been motivating fruitful design of more powerful deep learning algorithms. We present this survey to connect different isolated areas in deep learning with their relation to transferability, and to provide a unified and complete view to investigating transferability through the whole lifecycle of deep learning.",
    "pdf_url": "https://arxiv.org/pdf/2201.05867v1",
    "source": "arxiv",
    "tags": [
      "cs.LG"
    ]
  },
  {
    "title": "How good are variational autoencoders at transfer learning?",
    "authors": [
      "Lisa Bonheme",
      "Marek Grzes"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2304.10767v1",
    "abstract": "Variational autoencoders (VAEs) are used for transfer learning across various research domains such as music generation or medical image analysis. However, there is no principled way to assess before transfer which components to retrain or whether transfer learning is likely to help on a target task. We propose to explore this question through the lens of representational similarity.",
    "pdf_url": "https://arxiv.org/pdf/2304.10767v1",
    "source": "arxiv",
    "tags": [
      "cs.LG"
    ]
  },
  {
    "title": "Transfer Learning for Kernel-based Regression",
    "authors": [
      "Chao Wang",
      "Caixing Wang",
      "Xin He",
      "Xingdong Feng"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2310.13966v3",
    "abstract": "In recent years, transfer learning has garnered significant attention. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space.",
    "pdf_url": "https://arxiv.org/pdf/2310.13966v3",
    "source": "arxiv",
    "tags": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ]
  },
  {
    "title": "G5: A Universal GRAPH-BERT for Graph-to-Graph Transfer and Apocalypse Learning",
    "authors": [
      "Jiawei Zhang"
    ],
    "year": 2020,
    "url": "http://arxiv.org/abs/2006.06183v1",
    "abstract": "The recent GRAPH-BERT model introduces a new approach to learning graph representations merely based on the attention mechanism. GRAPH-BERT provides an opportunity for transferring pre-trained models and learned graph representations across different tasks within the same graph dataset. In this paper, we will further investigate the graph-to-graph transfer of a universal GRAPH-BERT for graph representation learning across different graph datasets.",
    "pdf_url": "https://arxiv.org/pdf/2006.06183v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.NE",
      "cs.SI",
      "stat.ML"
    ]
  },
  {
    "title": "Transfer Learning between Motor Imagery Datasets using Deep Learning -- Validation of Framework and Comparison of Datasets",
    "authors": [
      "Pierre Guetschel",
      "Michael Tangermann"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2311.16109v1",
    "abstract": "We present a simple deep learning-based framework commonly used in computer vision and demonstrate its effectiveness for cross-dataset transfer learning in mental imagery decoding tasks that are common in the field of Brain-Computer Interfaces (BCI).",
    "pdf_url": "https://arxiv.org/pdf/2311.16109v1",
    "source": "arxiv",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "title": "Transfer learning in hybrid classical-quantum neural networks",
    "authors": [
      "Andrea Mari",
      "Thomas R. Bromley",
      "Josh Izaac",
      "Maria Schuld",
      "Nathan Killoran"
    ],
    "year": 2019,
    "url": "http://arxiv.org/abs/1912.08278v2",
    "abstract": "We extend the concept of transfer learning, widely applied in modern machine learning algorithms, to the emerging context of hybrid neural networks composed of classical and quantum elements. We propose different implementations of hybrid transfer learning, but we focus mainly on the paradigm in which a pre-trained classical network is modified and augmented by a final variational quantum circuit.",
    "pdf_url": "https://arxiv.org/pdf/1912.08278v2",
    "source": "arxiv",
    "tags": [
      "quant-ph",
      "cs.LG",
      "stat.ML"
    ],
    "doi": "10.22331/q-2020-10-09-340"
  },
  {
    "title": "Transfer Learning for Causal Effect Estimation",
    "authors": [
      "Song Wei",
      "Hanyu Zhang",
      "Ronald Moore",
      "Rishikesan Kamaleswaran",
      "Yao Xie"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2305.09126v3",
    "abstract": "We present a Transfer Causal Learning (TCL) framework when target and source domains share the same covariate/feature spaces, aiming to improve causal effect estimation accuracy in limited data.",
    "pdf_url": "https://arxiv.org/pdf/2305.09126v3",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.ML"
    ]
  },
  {
    "title": "Communication-Efficient and Privacy-Preserving Feature-based Federated Transfer Learning",
    "authors": [
      "Feng Wang",
      "M. Cenk Gursoy",
      "Senem Velipasalar"
    ],
    "year": 2022,
    "url": "http://arxiv.org/abs/2209.05395v1",
    "abstract": "Federated learning has attracted growing interest as it preserves the clients' privacy. As a variant of federated learning, federated transfer learning utilizes the knowledge from similar tasks and thus has also been intensively studied. However, due to the limited radio spectrum, the communication efficiency of federated learning via wireless links is critical since some tasks may require thousands of Terabytes of uplink payload.",
    "pdf_url": "https://arxiv.org/pdf/2209.05395v1",
    "source": "arxiv",
    "tags": [
      "cs.LG"
    ]
  },
  {
    "title": "Efficient Transfer Learning via Joint Adaptation of Network Architecture and Weight",
    "authors": [
      "Ming Sun",
      "Haoxuan Dou",
      "Junjie Yan"
    ],
    "year": 2021,
    "url": "http://arxiv.org/abs/2105.08994v1",
    "abstract": "Transfer learning can boost the performance on the target task by leveraging the knowledge of the source domain. Recent works in neural architecture search (NAS), especially one-shot NAS, can aid transfer learning by establishing sufficient network search space.",
    "pdf_url": "https://arxiv.org/pdf/2105.08994v1",
    "source": "arxiv",
    "tags": [
      "cs.CV"
    ]
  },
  {
    "title": "Transfer Representation Learning with TSK Fuzzy System",
    "authors": [
      "Peng Xu",
      "Zhaohong Deng",
      "Jun Wang",
      "Qun Zhang",
      "Shitong Wang"
    ],
    "year": 2019,
    "url": "http://arxiv.org/abs/1901.02703v1",
    "abstract": "Transfer learning can address the learning tasks of unlabeled data in the target domain by leveraging plenty of labeled data from a different but related source domain. A core issue in transfer learning is to learn a shared feature space in where the distributions of the data from two domains are matched.",
    "pdf_url": "https://arxiv.org/pdf/1901.02703v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation",
    "authors": [
      "Shani Gamrian",
      "Yoav Goldberg"
    ],
    "year": 2018,
    "url": "http://arxiv.org/abs/1806.07377v6",
    "abstract": "Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior.",
    "pdf_url": "https://arxiv.org/pdf/1806.07377v6",
    "source": "arxiv",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  }
]
