[
  {
    "title": "The Modern Mathematics of Deep Learning",
    "authors": [
      "Julius Berner",
      "Philipp Grohs",
      "Gitta Kutyniok",
      "Philipp Petersen"
    ],
    "year": 2021,
    "url": "http://arxiv.org/abs/2105.04026v2",
    "abstract": "We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.",
    "pdf_url": "https://arxiv.org/pdf/2105.04026v2",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "stat.ML"
    ],
    "doi": "10.1017/9781009025096.002"
  },
  {
    "title": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice",
    "authors": [
      "Deep Pandey",
      "Qi Yu"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2306.11113v2",
    "abstract": "Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. We theoretically investigate evidential models and identify a fundamental limitation: existing evidential activation functions create zero evidence regions, which prevent the model from learning from training samples falling into such regions.",
    "pdf_url": "https://arxiv.org/pdf/2306.11113v2",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "title": "Deep Learning and Computational Physics (Lecture Notes)",
    "authors": [
      "Deep Ray",
      "Orazio Pinti",
      "Assad A. Oberai"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2301.00942v1",
    "abstract": "These notes were compiled as lecture notes for a course developed and taught at the University of the Southern California. They should be accessible to a typical engineering graduate student with a strong background in Applied Mathematics. The main objective of these notes is to introduce a student who is familiar with concepts in linear algebra and partial differential equations to select topics in deep learning. These lecture notes exploit the strong connections between deep learning algorithms and the more conventional techniques of computational physics to achieve two goals. First, they use concepts from computational physics to develop an understanding of deep learning algorithms. Second, several novel deep learning algorithms can be used to solve challenging problems in computational physics.",
    "pdf_url": "https://arxiv.org/pdf/2301.00942v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "math-ph"
    ]
  },
  {
    "title": "Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation",
    "authors": [
      "Deep Shankar Pandey",
      "Hyomin Choi",
      "Qi Yu"
    ],
    "year": 2025,
    "url": "http://arxiv.org/abs/2512.23753v1",
    "abstract": "Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty-aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective-Logic framework constrains evidence to be non-negative, requiring specific activation functions whose geometric properties can induce activation-dependent learning-freeze behavior: a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers.",
    "pdf_url": "https://arxiv.org/pdf/2512.23753v1",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "Quadratic number of nodes is sufficient to learn a dataset via gradient descent",
    "authors": [
      "Biswarup Das",
      "Eugene. A. Golikov"
    ],
    "year": 2019,
    "url": "http://arxiv.org/abs/1911.05402v1",
    "abstract": "We prove that if an activation function satisfies some mild conditions and number of neurons in a two-layered fully connected neural network with this activation function is beyond a certain threshold, then gradient descent on quadratic loss function finds the optimal weights of input layer for global minima in linear time. This threshold value is an improvement over previously obtained values. We hypothesise that this bound cannot be improved by the method we are using in this work.",
    "pdf_url": "https://arxiv.org/pdf/1911.05402v1",
    "source": "arxiv",
    "tags": [
      "math.OC",
      "cs.LG",
      "math.ST"
    ]
  },
  {
    "title": "Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time",
    "authors": [
      "Arvind Mahankali",
      "Jeff Z. Haochen",
      "Kefan Dong",
      "Margalit Glasgow",
      "Tengyu Ma"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2306.16361v2",
    "abstract": "Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size n = O(d^3.1) where d is the dimension of the inputs, the network trained with projected gradient flow converges in poly(d) time to a non-trivial error that is not achievable by kernel methods, hence demonstrating a clear separation between unmodified gradient descent and NTK.",
    "pdf_url": "https://arxiv.org/pdf/2306.16361v2",
    "source": "arxiv",
    "tags": [
      "cs.LG"
    ]
  },
  {
    "title": "Survey Descent: A Multipoint Generalization of Gradient Descent for Nonsmooth Optimization",
    "authors": [
      "X. Y. Han",
      "Adrian S. Lewis"
    ],
    "year": 2021,
    "url": "http://arxiv.org/abs/2111.15645v5",
    "abstract": "For strongly convex objectives that are smooth, the classical theory of gradient descent ensures linear convergence relative to the number of gradient evaluations. An analogous nonsmooth theory is challenging. We propose a multipoint generalization of the gradient descent iteration for local optimization. While designed with general objectives in mind, we are motivated by a 'max-of-smooth' model that captures the subdifferential dimension at optimality. We prove linear convergence when the objective is itself max-of-smooth, and experiments suggest a more general phenomenon.",
    "pdf_url": "https://arxiv.org/pdf/2111.15645v5",
    "source": "arxiv",
    "tags": [
      "math.OC",
      "cs.CG",
      "cs.LG",
      "math.NA"
    ],
    "doi": "10.1137/21M1468450"
  },
  {
    "title": "Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks",
    "authors": [
      "Danny D'Agostino",
      "Ilija Ilievski",
      "Christine Annette Shoemaker"
    ],
    "year": 2023,
    "url": "http://arxiv.org/abs/2307.05639v2",
    "abstract": "Providing a model that achieves a strong predictive performance and is simultaneously interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. We propose a modification of the radial basis function neural network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction.",
    "pdf_url": "https://arxiv.org/pdf/2307.05639v2",
    "source": "arxiv",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "doi": "10.1016/j.neunet.2024.106335"
  }
]
