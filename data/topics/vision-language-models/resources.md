# Ressources — Vision-Language Models

## Cours en ligne
- [Stanford CS231n — Deep Learning for Computer Vision](http://cs231n.stanford.edu/)
- [Lil'Log — Generalist Vision-Language Models](https://lilianweng.github.io/posts/2022-06-09-vlm/)
- [Hugging Face — Vision-Language Models Explained](https://huggingface.co/blog/vlms)

## Livres
- *Deep Learning* — Ian Goodfellow, Yoshua Bengio, Aaron Courville
- *Multimodal Deep Learning* — revues et tutoriels ACL/CVPR

## Outils
- [Hugging Face Transformers](https://huggingface.co/transformers/) — CLIP, BLIP, LLaVA, etc.
- [OpenCLIP](https://github.com/mlfoundations/open_clip) — implémentation open-source de CLIP
- [LLaVA](https://llava-vl.github.io/) — Visual instruction tuning
- [Gradio](https://gradio.app/) — démos interactives pour modèles multimodaux
